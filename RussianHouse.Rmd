---
title: "Russian Housing Market"
output: pdf_document
---
# Russian Housing Market

```{r}
library(tidyverse)
library(caret)
library(DataExplorer)
library(rgdal)
library(RColorBrewer)
library(sp)
library(lubridate)
library(plotly)
library(ggthemes)
library(mapproj)
library(ggiraph)
library(scales)
```
## Exploratory Data Analysis

### Training and Test Set

I will read in the training and test sets and then look to see how many missing values and which variables have missing values. I will see if it is similar between the test and training sets as well. 
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")

plot_missing(train[, 1:50])
plot_missing(train[, 51:100])
plot_missing(train[,101:150])
plot_missing(train[,151:200])
plot_missing(train[,201:250])
plot_missing(train[,251:292])

# Although these plots are easy, there are too many variables to have a single plot. As an alternative, I will calculate the percentage of missing values for each variable and then plot the ones that are greater than 0. 

missing_vals <- function(x) {
   value <-  round((sum(is.na(x))/length(x)) * 100, digits = 2) 
   return(value)
}
miss_pct <- map_dbl(train, missing_vals)

miss_pct.df <- data.frame(pct = miss_pct, Variable = names(miss_pct)) %>% 
    filter(pct > 0)

ggplot(miss_pct.df, aes(x = reorder(Variable, pct), y = pct)) + 
    geom_bar(stat = "identity", fill = "#DA291C") +
    labs(y = "Percentage of missing values", x = "Variable") +
    coord_flip()
```
There are a total of 292 columns in the training set. There are 41 variables that have more than 1% of values missing. There are 10 variables that have missing values, but 1% or less of the observations are missing that value. All 51 categories that have missing values are numeric variables, so there could be values imputed for all of those variables. There are 276 numeric variables and 15 categorical variables and 1 variable that should be a date variable. As a result, I will change the timestamp to a date variable later. 

```{r}
plot_missing(test[, 1:50])
plot_missing(test[, 51:100])
plot_missing(test[,101:150])
plot_missing(test[,151:200])
plot_missing(test[,201:250])
plot_missing(test[,251:291])

# Although these plots are easy, there are too many variables to have a single plot. As an alternative, I will calculate the percentage of missing values for each variable and then plot the ones that are greater than 0. 

miss_pct_test <- map_dbl(test, missing_vals)

miss_pct_test.df <- data.frame(pct = miss_pct_test, Variable = names(miss_pct_test)) %>% 
    filter(pct > 0)

ggplot(miss_pct_test.df, aes(x = reorder(Variable, pct), y = pct)) + 
    geom_bar(stat = "identity", fill = "#DA291C") +
    labs(y = "Percentage of missing values", x = "Variable") +
    coord_flip()
```
There are 291 variables in the test set since there is not a variable for the house price. There are 48 variables that have missing values in the test set and 47 of those variables are numeric and one variable is a categorical variable that has missing values. Since only the test set includes missing values for the one categorical variable, I will need to determine if I want to impute values or not include the variable in which ever model is fit to the data. 

## Variable Exploration

### Response Variable

Next, I want to look at the distribution of the response variable
```{r}
ggplot(train, aes(x = price_doc)) + geom_histogram(fill = "firebrick")
```

The distribution is skewed right, which isn't surprised when dealing with house prices. Most houses will be a similar price, but there are a small number of houses that are extremely expensive. I will likely use a model that uses the natural log of the sale price becuase it won't be as heavily influenced by the outliers. 

There are still a few outliers, but the distribution of sale price looks a lot better than before. 

```{r}
ggplot(train, aes(x= log(price_doc))) + geom_histogram(fill = "firebrick")
```



```{r}
train$timestamp <- ymd(train$timestamp)
test$timestamp <- ymd(test$timestamp)
train <- train %>%
  mutate(year = year(timestamp),
         month = month(timestamp, label = TRUE, abbr = FALSE),
         weekday = weekdays(timestamp),
         day = day(timestamp))

p1 <- train %>%
  group_by(year) %>%
  summarize(med_price = median(price_doc)) %>%
  ggplot(aes(x = factor(year), y = med_price, fill = factor(year))) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d() +
  labs(x = "Year", y = "Median Price") +
  scale_y_continuous(labels = comma) +
  theme_pander() +
  theme(legend.position = "none")

p2 <-  train %>%
  group_by(month) %>%
  summarize(med_price = median(price_doc)) %>%
  ggplot(aes(x = factor(month), y = med_price, fill = factor(month))) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d() +
  labs(x = "Month", y = "Median Price") +
  scale_y_continuous(labels = comma) +
  theme_pander() +
  theme(legend.position = "none")


p3 <-  train %>%
  group_by(weekday) %>%
  summarize(med_price = median(price_doc),
            count = n()) %>%
  ggplot(aes(x = factor(weekday), y = count, fill = factor(weekday))) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d() +
  labs(x = "Month", y = "Median Price") +
  scale_y_continuous(labels = comma) +
  theme_pander() +
  theme(legend.position = "none")
p1
p2
p3
```


The `full_sq` and `life_sq` variables should be strongly correlated and based off the definitions of these variables, `full_sq` should always be equal to or greater than `life_sq`.
```{r}
sum(train$life_sq > train$full_sq, na.rm = TRUE)

train[train$life_sq > train$full_sq, ] %>% filter(!is.na(full_sq)) %>%
  ggplot(aes(x = full_sq, y = life_sq)) + geom_point()
```

There are 37 houses that have the `life_sq` which is greater than the `full_sq`, which is not possible according to the definitions of these variables, so I will remove those observations and see what happens. 

I want to visualize the data on maps, so I'm going to start my EDA by doing that. I'm following a lot of this notebook (https://www.kaggle.com/jtremoureux/map-visualizations-with-external-shapefile), but hopefully will do some of my own stuff as well. 

## Map Visualizations

```{r}
districts <- read_delim("districts.csv", delim = ";") %>% 
    select(ID, `English Name`, `Russian Name`)
areas <- read_delim("regions.csv", delim = ";")
shp <- readOGR(dsn = "moscow_districts/", layer = "moscow_adm")

# Get the box for polygons
bbox <- shp@bbox

# Get centroids
centroids <- coordinates(shp)
mo_shp <- shp
mo_shp$long_c <- centroids[,1]
mo_shp$lat_c <- centroids[,2]
mo_data <- mo_shp@data
mo_data <- mo_data %>% 
    left_join(districts, by = c("OKRUGS" = "English Name")) %>% 
    left_join(areas, by = c("RAION" = "English"))
mo_shp$district <- mo_data$`Russian Name`
mo_shp$area <- mo_data$Russian
mo_shp$district <- factor(mo_shp$district)
mo_shp$area <- factor(mo_shp$area)
head(mo_shp)
```

```{r}
okrugs_pal <- brewer.pal(12, "Paired") # Define a nice color palette with RColorBrewer
plot(mo_shp, border = "#57595b", col = okrugs_pal[as.numeric(mo_shp$district)], main = "Округи и Районы")
points(centroids, col = "black", pch = 20, cex = 0.3)
legend("right", title = "Округ", legend = sort(unique(mo_shp$district)),  fill = okrugs_pal, border = "#57595b", cex=0.8)
```

```{r}
# These plots can be interchanged between Russian and English names. I hate transliterated Russian, so almost all plots that I do are in Russian, but all that needs to be done is to switch the column used in the dataset; District corresponds with OKRUGS and area with RAION
spplot(mo_shp, c("district"), main = "Округи и районы Москвы", col.regions=okrugs_pal)
```

I will add some summary characteristics of houses in each area
```{r}
head(train)

area_frame <- train %>%
    group_by(sub_area) %>%
    summarize(MedYr = median(build_year, na.rm = TRUE),
              AvgSalePrice = mean(price_doc),
              AvgSqFt = mean(full_sq), 
              PriceSqft = mean(price_doc/full_sq),
              Floors = mean(floor),
              AvgPop = mean(raion_popul),
              AvgInd = mean(indust_part),
              AvgWorkAge = mean(work_all),
              Bld19 = mean(build_count_after_1995))
str(train)
area_frame
head(area_frame)
head(mo_shp)
mo_data <- mo_data %>% left_join(area_frame, by = c("RAION" = "sub_area"))
area_frame
head(mo_data)
mo_shp@data <- mo_data
mo_shp$build_year <- mo_data$MedYr
mo_shp$AvgSalePrice <- mo_data$AvgSalePrice
mo_shp$AvgSqFt
head(mo_shp)
ncolors <- 15
pal3 <- colorRampPalette(c("grey90", "red"))(ncolors)
spplot(mo_shp, c("AvgPop"), main = "Transaction prices", col.regions = pal3,
       sub = "", cuts = ncolors-1, col = "transparent")
```

```{r}

data_area <- train %>%
  filter(!duplicated(train$sub_area)) %>% # select one row for each area
  select(sub_area:build_count_after_1995) # select the area properties
# Add it to our SPDF

mo_data <- mo_data %>% full_join(data_area, by = c("RAION" = "sub_area"))
mo_shp@data <- mo_data


mo_df <- mo_shp %>% fortify(region = 'Russian')
mo_df$id <- as.factor(mo_df$id)
head(mo_df)
gg_df <- full_join(mo_df, mo_shp@data, by = c("id" = "Russian"))

head(gg_df)
p2 <- ggplot(data=gg_df, mapping=aes(x=long, y=lat, group=group, fill=log(AvgSalePrice))) + geom_polygon(color=NA) + scale_fill_distiller(palette="RdBu") + xlab("Lon") + ylab("Lat") + labs(fill="Log(Sale Price)") + coord_map()

ggplotly(p2)
p1 <- ggplot(gg_df, aes(long, lat, group=group, fill=green_zone_part)) +
  theme_classic() +
  geom_polygon() +
  geom_path(color="white", size = 0.05) +
  coord_map() + theme_tufte(ticks = FALSE) + 
  scale_fill_gradient(low = "#e5f5e0", high = "#00441b") +
  theme(axis.text = element_blank(), axis.title = element_blank())
p1
```

```{r}
p <- ggplot(gg_df, aes(long, lat, group=group)) + theme_classic() +
  geom_polygon_interactive(aes(fill=raion_popul, # javascript to run on click
                               data_id = id)) + # column associated to polygon element
  coord_map() + theme_tufte(ticks = FALSE) +
  scale_fill_continuous(low = "#f7f4f9", high = "#980043") +
  theme(axis.text = element_blank(), axis.title = element_blank(),
        plot.title = element_text(hjust = 0.8, face = "bold")) +
  ggtitle(" Click on the map! 👌 ")
p
```

## Variable Importance

```{r}


#Get complete cases of dtrain
completes <- complete.cases(train)

# Set training control so that we only 1 run forest on the entire set of complete cases
trControl <- trainControl(method='none')

# Run random forest on complete cases of dtrain. Exclude incineration_raion since it
# only has 1 factor level
rfmod <- train(price_doc ~ . - id - timestamp - incineration_raion,
               method='rf',
               data=train[completes, ],
               trControl=trControl,
               tuneLength=1,
               importance=TRUE)
varImp(rfmod)
?varImp()
```
```{r}
macro <- read.csv("macro.csv")
head(macro)
```


## Model Fitting

```{r}
custom_summary <- function(data, lev = NULL, model = NULL) {
out <- Metrics::rmsle(data[, "obs"], data[, "pred"])
names(out) <- c("rmsle")
out
}
myControl <- trainControl(method = "cv",
                          number = 3,
                          summaryFunction = custom_summary)

train.model <- train %>% select_if(is.numeric)
test.model <- test %>% select_if(is.numeric)

impute <- preProcess(train.model, "medianImpute")
train.model <- predict(impute, train.model)
impute.test <- preProcess(test.model, "medianImpute")
test.model <- predict(impute.test, test.model)


tunegrid <- expand.grid(eta = .2,
                        max_depth = 3,
                        colsample_bytree = .75,
                        subsample = .95,
                        nrounds =  400,
                        min_child_weight = 5,
                        gamma = .075)
set.seed(4171996)
xgbTree.model <- train(log(price_doc)~.,
                   data = train.model,
                   method = "xgbTree",
                   tuneGrid = tunegrid,
                   trControl = myControl,
                   metric = "rmsle",
                   maximize = FALSE,
                   preProc = c("center", "scale")
)
beepr::beep(sound = 8)
xgbTree.model
preds <- predict(xgbTree.model, test.model) %>% exp()
dat <- data.frame(id = test$id, price_doc = preds)
write_csv(dat, "xgbTree-num2.csv")
```

```{r}
custom_summary <- function(data, lev = NULL, model = NULL) {
out <- Metrics::rmsle(data[, "obs"], data[, "pred"])
names(out) <- c("rmsle")
out
}
myControl <- trainControl(method = "cv",
                          number = 10,
                          summaryFunction = custom_summary)

small.data <- train %>% select(full_sq, life_sq, num_room, build_year, max_floor, state, kitch_sq, nuclear_reactor_km, floor, thermal_power_plant_km, railroad_station_walk_km, green_part_1000, hospice_morgue_km, prom_part_1000, mkad_km, metro_min_walk, office_km, cafe_count_5000_price_2500, industrial_km, price_doc)

small.test <- test %>% select(full_sq, life_sq, num_room, build_year, max_floor, state, kitch_sq, nuclear_reactor_km, floor, thermal_power_plant_km, railroad_station_walk_km, green_part_1000, hospice_morgue_km, prom_part_1000, mkad_km, metro_min_walk, office_km, cafe_count_5000_price_2500, industrial_km)

plot_missing(small.data)
plot_missing(small.test)
set.seed(15)
tunegrid <- expand.grid(eta = c(.2,.25,.3),
                        max_depth = c(1, 3, 5, 9),
                        colsample_bytree = .9,
                        subsample = .8,
                        nrounds = 100,
                        min_child_weight = 1,
                        gamma = .075)
impute <- preProcess(small.data, "medianImpute")
small.data2 <- predict(impute, small.data)

impute.test <- preProcess(small.test, "medianImpute")
small.test2 <- predict(impute.test, small.test)

xgbTree.model <- train(log(price_doc)~.,
                   data = small.data2,
                   method = "xgbTree",
                   tuneGrid = tunegrid,
                   trControl = myControl,
                   metric = "rmsle",
                   maximize = FALSE,
                   preProc = c("center", "scale")
)
xgbTree.model
head(small.test2)
preds <- predict(xgbTree.model, small.test2) %>% exp()
dat <- data.frame(id = test$id, price_doc = preds)
write_csv(dat, "xgbTree-many.csv")
```

